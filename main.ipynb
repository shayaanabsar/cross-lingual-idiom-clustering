{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import csv\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14673, 24])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "translation_clusters = {}\n",
    "idioms = set()\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "with open('dataset.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        idiom = row['english']\n",
    "        translation = row['german']\n",
    "        idioms.add(idiom)\n",
    "        idioms.add(translation)\n",
    "\n",
    "        if idiom in translation_clusters:\n",
    "            translation_clusters[idiom].add(translation)\n",
    "        else:\n",
    "            translation_clusters[idiom] = {translation}\n",
    "\n",
    "        if translation in translation_clusters:\n",
    "            translation_clusters[translation].add(idiom)\n",
    "        else:\n",
    "            translation_clusters[translation] = {idiom}\n",
    "\n",
    "# Convert idioms to tokenized representations\n",
    "max_length = 0\n",
    "encoder = {}\n",
    "\n",
    "for idiom in idioms:\n",
    "    encoded = tokenizer(idiom, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "    max_length = max(max_length, encoded.shape[1])\n",
    "    encoder[idiom] = encoded\n",
    "\n",
    "num_idioms = len(idioms)\n",
    "decoder = {}\n",
    "\n",
    "# Generate idiom tensor\n",
    "idiom_tensor = torch.zeros(size=(num_idioms, max_length), dtype=torch.long)\n",
    "for i, idiom in enumerate(idioms):\n",
    "    encoded = encoder[idiom]\n",
    "    idiom_tensor[i, :encoded.shape[1]] = encoded[0]\n",
    "    encoder[idiom] = idiom_tensor[i]\n",
    "    decoder[tuple(idiom_tensor[i].tolist())] = idiom\n",
    "\n",
    "assert decoder[tuple(encoder['makes me feel like'].tolist())] == 'makes me feel like'\n",
    "\n",
    "# Convert idiom clusters to tokenized representations\n",
    "translation_clusters_tokenized = {}\n",
    "for idiom in translation_clusters:\n",
    "    translation_clusters_tokenized[tuple(encoder[idiom].tolist())] = set()\n",
    "    for match in translation_clusters[idiom]:\n",
    "        if tuple(encoder[match].tolist()) == tuple(encoder[idiom].tolist()): continue\n",
    "        translation_clusters_tokenized[tuple(encoder[idiom].tolist())].add(tuple(encoder[match].tolist()))\n",
    "\n",
    "# Print the shape of idiom_tensor\n",
    "print(idiom_tensor.shape)\n",
    "\n",
    "translation_clusters = translation_clusters_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = idiom_tensor[:int(0.9*num_idioms)]\n",
    "test = idiom_tensor[int(0.9*num_idioms):int(0.95*num_idioms)]\n",
    "val = idiom_tensor[int(0.95*num_idioms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dimensions = 2\n",
    "device = 'mps'\n",
    "iterations = 2000\n",
    "learning_rate = 0.01\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_set(tensor):\n",
    "    return {tuple(d.tolist()) for d in tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 24])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data):\n",
    "\tindexes = torch.randint(0, len(data), (batch_size,))\n",
    "\tbatch = set()\n",
    "\tset_data = tensor_to_set(data)\n",
    "\n",
    "\tfor i in indexes:\n",
    "\t\tidiom = data[i]\n",
    "\n",
    "\t\tpossible_idioms = set_data.intersection(translation_clusters[tuple(idiom.tolist())])\n",
    "\n",
    "\t\tif len(possible_idioms) == 0:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tassert not(tuple(idiom.tolist()) in possible_idioms)\n",
    "\n",
    "\t\trandom_cluster_mate = choice(list(possible_idioms))\n",
    "\t\tbatch.add(tuple(random_cluster_mate))\n",
    "\t\tbatch.add(tuple(idiom.tolist()))\n",
    "\n",
    "\tbatch = torch.tensor([list(x) for x in list(batch)]).to(device)\n",
    "\treturn batch\n",
    "\n",
    "print(get_batch(train).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_sample(data):\n",
    "    positive_samples = []\n",
    "\n",
    "    for anchor in data:\n",
    "        possible_positive = translation_clusters[tuple(anchor.tolist())].intersection(tensor_to_set(data))\n",
    "\n",
    "\n",
    "        chosen = torch.tensor(choice(list(possible_positive)))\n",
    "        positive_samples.append(chosen)\n",
    "\n",
    "    positive_samples = torch.stack(positive_samples).to(device)\n",
    "\n",
    "    return positive_samples\n",
    "\n",
    "\n",
    "def get_negative_sample(data):\n",
    "    negative_samples = []\n",
    "\n",
    "    for anchor in data:\n",
    "        possible_negative = tensor_to_set(data).difference(translation_clusters[tuple(anchor.tolist())])\n",
    "\n",
    "        chosen = torch.tensor(choice(list(possible_negative)))\n",
    "        negative_samples.append(chosen)\n",
    "\n",
    "    negative_samples = torch.stack(negative_samples).to(device)\n",
    "\n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self, pooling):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.pooling = pooling\n",
    "\t\tself.roberta = AutoModelForMaskedLM.from_pretrained('xlm-roberta-base')\n",
    "\t\tself.output_layer = nn.Linear(250002, latent_dimensions) \n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tinput_ids = torch.tensor(input, dtype=torch.long).to(device)\n",
    "\t\tattention_mask = torch.LongTensor(torch.ones(input.shape, dtype=torch.long)).to(device)\n",
    "\t\troberta_logits = self.roberta(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\t\tif self.pooling == 'average': pooled = torch.mean(roberta_logits, dim=0)\n",
    "\t\tvector_representation = self.output_layer(pooled)\n",
    "\t\t\n",
    "\t\treturn vector_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(pooling='average')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/ld6wd4rd4416tr7z2q29t7180000gn/T/ipykernel_57954/1542951500.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.06 GB, other allocations: 2.93 GB, max allowed: 9.07 GB). Tried to allocate 91.55 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m positive_samples \u001b[39m=\u001b[39m get_positive_sample(batch)\n\u001b[1;32m      4\u001b[0m negative_samples \u001b[39m=\u001b[39m get_negative_sample(batch)\n\u001b[0;32m----> 6\u001b[0m encodings \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m      7\u001b[0m positive_sample_encodings \u001b[39m=\u001b[39m model(positive_samples)\n\u001b[1;32m      8\u001b[0m negative_sample_encodings \u001b[39m=\u001b[39m model(negative_samples)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[71], line 11\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      9\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39minput\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(torch\u001b[39m.\u001b[39mones(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m roberta_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39maverage\u001b[39m\u001b[39m'\u001b[39m: pooled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(roberta_logits, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m vector_representation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(pooled)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1120\u001b[0m, in \u001b[0;36mXLMRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1106\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroberta(\n\u001b[1;32m   1107\u001b[0m     input_ids,\n\u001b[1;32m   1108\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1118\u001b[0m )\n\u001b[1;32m   1119\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1120\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(sequence_output)\n\u001b[1;32m   1122\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1158\u001b[0m, in \u001b[0;36mXLMRobertaLMHead.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[1;32m   1157\u001b[0m \u001b[39m# project back to size of vocabulary with bias\u001b[39;00m\n\u001b[0;32m-> 1158\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(x)\n\u001b[1;32m   1160\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 6.06 GB, other allocations: 2.93 GB, max allowed: 9.07 GB). Tried to allocate 91.55 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "\tbatch = get_batch(train)\n",
    "\tpositive_samples = get_positive_sample(batch)\n",
    "\tnegative_samples = get_negative_sample(batch)\n",
    "\n",
    "\tencodings = model(batch)\n",
    "\tpositive_sample_encodings = model(positive_samples)\n",
    "\tnegative_sample_encodings = model(negative_samples)\n",
    "\n",
    "\ttriplet_loss = nn.TripletMarginLoss()\n",
    "\tloss = triplet_loss(encodings, positive_sample_encodings, negative_sample_encodings)\n",
    "\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tprint(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
