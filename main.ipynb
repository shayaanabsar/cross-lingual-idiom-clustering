{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:30:56.591807Z",
     "iopub.status.busy": "2023-07-07T19:30:56.590557Z",
     "iopub.status.idle": "2023-07-07T19:31:01.449955Z",
     "shell.execute_reply": "2023-07-07T19:31:01.448900Z",
     "shell.execute_reply.started": "2023-07-07T19:30:56.591738Z"
    },
    "id": "h6WigLZWPURc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (4.21.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.12.1+cu116)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (0.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (5.9.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (0.1.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (66.1.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (3.19.6)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (1.0.11)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (3.1.30)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb->-r requirements.txt (line 3)) (8.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 3)) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 3)) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import csv\n",
    "from random import choice\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:01.452590Z",
     "iopub.status.busy": "2023-07-07T19:31:01.452313Z",
     "iopub.status.idle": "2023-07-07T19:31:01.466582Z",
     "shell.execute_reply": "2023-07-07T19:31:01.465018Z",
     "shell.execute_reply.started": "2023-07-07T19:31:01.452557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:01.469229Z",
     "iopub.status.busy": "2023-07-07T19:31:01.468738Z",
     "iopub.status.idle": "2023-07-07T19:31:06.914443Z",
     "shell.execute_reply": "2023-07-07T19:31:06.913379Z",
     "shell.execute_reply.started": "2023-07-07T19:31:01.469188Z"
    },
    "id": "1cP8UKxkPURd",
    "outputId": "1bc8381b-cf0c-484f-8ded-88d23c1648c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b2027776ac4557a23c01887bd4cad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af829f185344df797381b6c07ec7cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading sentencepiece.bpe.model:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da73f34de454b54810d1de1923b7218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14673, 24])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "translation_clusters = {}\n",
    "idioms = set()\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "with open('dataset.csv') as f:\n",
    "\treader = csv.DictReader(f)\n",
    "\tfor row in reader:\n",
    "\t\tidiom = row['english']\n",
    "\t\ttranslation = row['german']\n",
    "\t\tidioms.add(idiom)\n",
    "\t\tidioms.add(translation)\n",
    "\n",
    "\t\tif idiom in translation_clusters:\n",
    "\t\t\ttranslation_clusters[idiom].add(translation)\n",
    "\t\telse:\n",
    "\t\t\ttranslation_clusters[idiom] = {translation}\n",
    "\n",
    "\t\tif translation in translation_clusters:\n",
    "\t\t\ttranslation_clusters[translation].add(idiom)\n",
    "\t\telse:\n",
    "\t\t\ttranslation_clusters[translation] = {idiom}\n",
    "\n",
    "# Convert idioms to tokenized representations\n",
    "max_length = 0\n",
    "encoder = {}\n",
    "\n",
    "for idiom in idioms:\n",
    "\tencoded = tokenizer(idiom, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\tmax_length = max(max_length, encoded.shape[1])\n",
    "\tencoder[idiom] = encoded\n",
    "\n",
    "num_idioms = len(idioms)\n",
    "decoder = {}\n",
    "\n",
    "# Generate idiom tensor\n",
    "idiom_tensor = torch.zeros(size=(num_idioms, max_length), dtype=torch.long)\n",
    "for i, idiom in enumerate(idioms):\n",
    "\tencoded = encoder[idiom]\n",
    "\tidiom_tensor[i, :encoded.shape[1]] = encoded[0]\n",
    "\tencoder[idiom] = idiom_tensor[i]\n",
    "\tdecoder[tuple(idiom_tensor[i].tolist())] = idiom\n",
    "\n",
    "assert decoder[tuple(encoder['makes me feel like'].tolist())] == 'makes me feel like'\n",
    "\n",
    "# Convert idiom clusters to tokenized representations\n",
    "translation_clusters_tokenized = {}\n",
    "for idiom in translation_clusters:\n",
    "\ttranslation_clusters_tokenized[tuple(encoder[idiom].tolist())] = set()\n",
    "\tfor match in translation_clusters[idiom]:\n",
    "\t\tif tuple(encoder[match].tolist()) == tuple(encoder[idiom].tolist()): continue\n",
    "\t\ttranslation_clusters_tokenized[tuple(encoder[idiom].tolist())].add(tuple(encoder[match].tolist()))\n",
    "\n",
    "# Print the shape of idiom_tensor\n",
    "print(idiom_tensor.shape)\n",
    "\n",
    "translation_clusters = translation_clusters_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:06.916864Z",
     "iopub.status.busy": "2023-07-07T19:31:06.916331Z",
     "iopub.status.idle": "2023-07-07T19:31:06.926197Z",
     "shell.execute_reply": "2023-07-07T19:31:06.924488Z",
     "shell.execute_reply.started": "2023-07-07T19:31:06.916822Z"
    },
    "id": "q3aT53B7PURe"
   },
   "outputs": [],
   "source": [
    "train = idiom_tensor[:int(0.9*num_idioms)]\n",
    "test = idiom_tensor[int(0.9*num_idioms):int(0.95*num_idioms)]\n",
    "val = idiom_tensor[int(0.95*num_idioms):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:32:50.837312Z",
     "iopub.status.busy": "2023-07-07T19:32:50.836885Z",
     "iopub.status.idle": "2023-07-07T19:32:51.446579Z",
     "shell.execute_reply": "2023-07-07T19:32:51.445270Z",
     "shell.execute_reply.started": "2023-07-07T19:32:50.837284Z"
    },
    "id": "cKFBxmbOPURe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshayaan-absar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230712_221949-1yvkno1i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/shayaan-absar/Cross-Lingual-Idiom-Sense-Clustering/runs/1yvkno1i\" target=\"_blank\">mild-smoke-170</a></strong> to <a href=\"https://wandb.ai/shayaan-absar/Cross-Lingual-Idiom-Sense-Clustering\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/shayaan-absar/Cross-Lingual-Idiom-Sense-Clustering/runs/1yvkno1i?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdb1a06b7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dimensions = 64\n",
    "device = 'cuda'\n",
    "iterations = 500\n",
    "learning_rate = 0.00001\n",
    "batch_size = 32\n",
    "batch_accumulation = 1\n",
    "\n",
    "roberta_output_length = 250002\n",
    "\n",
    "wandb.init(\n",
    "\tproject=\"Cross-Lingual-Idiom-Sense-Clustering\",\n",
    "\t\n",
    "\t# track hyperparameters and run metadata\n",
    "\tconfig={\n",
    "\t\"learning_rate\": learning_rate,\n",
    "\t\"architecture\": \"BERT\",\n",
    "\t\"epochs\": iterations,\n",
    "\t\"embedding_dimensions\":latent_dimensions,\n",
    "\t}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:18.595736Z",
     "iopub.status.busy": "2023-07-07T19:31:18.594621Z",
     "iopub.status.idle": "2023-07-07T19:31:18.601770Z",
     "shell.execute_reply": "2023-07-07T19:31:18.600718Z",
     "shell.execute_reply.started": "2023-07-07T19:31:18.595701Z"
    },
    "id": "ekZ5dy91PURe"
   },
   "outputs": [],
   "source": [
    "def tensor_to_set(tensor):\n",
    "\treturn {tuple(d.tolist()) for d in tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:18.604409Z",
     "iopub.status.busy": "2023-07-07T19:31:18.603228Z",
     "iopub.status.idle": "2023-07-07T19:31:18.712110Z",
     "shell.execute_reply": "2023-07-07T19:31:18.710491Z",
     "shell.execute_reply.started": "2023-07-07T19:31:18.604367Z"
    },
    "id": "RfR89UMhPURe",
    "outputId": "468a1046-91d5-4095-d92d-b6d03eedf8b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 24])\n"
     ]
    }
   ],
   "source": [
    "def create_batch(data):\n",
    "\tindexes = torch.randint(0, len(data), (batch_size,))\n",
    "\tbatch = set()\n",
    "\tset_data = tensor_to_set(data)\n",
    "\n",
    "\tfor i in indexes:\n",
    "\t\tidiom = data[i]\n",
    "\n",
    "\t\tpossible_idioms = set_data.intersection(translation_clusters[tuple(idiom.tolist())])\n",
    "\n",
    "\t\tif len(possible_idioms) == 0:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tassert not(tuple(idiom.tolist()) in possible_idioms)\n",
    "\n",
    "\t\trandom_cluster_mate = choice(list(possible_idioms))\n",
    "\t\tbatch.add(tuple(random_cluster_mate))\n",
    "\t\tbatch.add(tuple(idiom.tolist()))\n",
    "\n",
    "\tbatch = torch.tensor([list(x) for x in list(batch)]).to(device)\n",
    "\n",
    "\treturn batch\n",
    "\n",
    "def get_batch(data):\n",
    "\tbatch = torch.tensor([])\n",
    "\n",
    "\twhile batch.numel() == 0:\n",
    "\t\tbatch = create_batch(data)\n",
    "\treturn batch\n",
    "\n",
    "print(get_batch(train).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:18.714224Z",
     "iopub.status.busy": "2023-07-07T19:31:18.713797Z",
     "iopub.status.idle": "2023-07-07T19:31:18.724347Z",
     "shell.execute_reply": "2023-07-07T19:31:18.722506Z",
     "shell.execute_reply.started": "2023-07-07T19:31:18.714184Z"
    },
    "id": "FooH8tw8PURe"
   },
   "outputs": [],
   "source": [
    "def get_positive_sample(data):\n",
    "\tpositive_samples = []\n",
    "\n",
    "\tfor anchor in data:\n",
    "\t\tpossible_positive = translation_clusters[tuple(anchor.tolist())].intersection(tensor_to_set(data))\n",
    "\n",
    "\n",
    "\t\tchosen = torch.tensor(choice(list(possible_positive)))\n",
    "\t\tpositive_samples.append(chosen)\n",
    "\n",
    "\tpositive_samples = torch.stack(positive_samples).to(device)\n",
    "\n",
    "\treturn positive_samples\n",
    "\n",
    "\n",
    "def get_negative_sample(data):\n",
    "\tnegative_samples = []\n",
    "\n",
    "\tfor anchor in data:\n",
    "\t\tpossible_negative = tensor_to_set(data).difference(translation_clusters[tuple(anchor.tolist())])\n",
    "\n",
    "\t\tchosen = torch.tensor(choice(list(possible_negative)))\n",
    "\t\tnegative_samples.append(chosen)\n",
    "\n",
    "\tnegative_samples = torch.stack(negative_samples).to(device)\n",
    "\n",
    "\treturn negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:18.726321Z",
     "iopub.status.busy": "2023-07-07T19:31:18.725861Z",
     "iopub.status.idle": "2023-07-07T19:31:18.735947Z",
     "shell.execute_reply": "2023-07-07T19:31:18.734702Z",
     "shell.execute_reply.started": "2023-07-07T19:31:18.726286Z"
    },
    "id": "anrHVnxyPURe"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self, pooling):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.pooling = pooling\n",
    "\t\tself.roberta = AutoModelForMaskedLM.from_pretrained('xlm-roberta-base')\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(max_length)\n",
    "\t\tself.output_layer = nn.Linear(roberta_output_length, latent_dimensions)\n",
    "\t\tself.activation = nn.LeakyReLU()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tinput_ids = torch.tensor(input, dtype=torch.long).to(device)\n",
    "\t\tattention_mask = torch.LongTensor(torch.ones(input.shape, dtype=torch.long)).to(device)\n",
    "\t\troberta_logits = self.roberta(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\t\tbatch_norm_roberta_logits = self.batch_norm(roberta_logits)\n",
    "\t\tif self.pooling == 'mean': pooled = torch.mean(batch_norm_roberta_logits, dim=1)\n",
    "\t\tif self.pooling == 'max': pooled, _ = torch.max(batch_norm_roberta_logits, dim=1)\n",
    "\t\tif self.pooling == 'min': pooled, _ = torch.min(batch_norm_roberta_logits, dim=1)\n",
    "\t\tvector_representation = self.output_layer(pooled)\n",
    "\t\tactivation = self.activation(vector_representation)\n",
    "\n",
    "\t\treturn activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-07T19:31:18.737933Z",
     "iopub.status.busy": "2023-07-07T19:31:18.737552Z",
     "iopub.status.idle": "2023-07-07T19:31:25.746753Z",
     "shell.execute_reply": "2023-07-07T19:31:25.745615Z",
     "shell.execute_reply.started": "2023-07-07T19:31:18.737897Z"
    },
    "id": "meFKOyz6PURf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1280558ab7fc4e2c9d9a581a6f18dbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model(pooling='mean')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2023-07-07T19:32:54.584387Z",
     "iopub.status.busy": "2023-07-07T19:32:54.584012Z",
     "iopub.status.idle": "2023-07-07T19:33:19.197930Z",
     "shell.execute_reply": "2023-07-07T19:33:19.196281Z",
     "shell.execute_reply.started": "2023-07-07T19:32:54.584357Z"
    },
    "id": "8TnNErnoPURf",
    "outputId": "655c6133-f8b5-4f31-8549-2ee708e16338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size=~64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/250995631.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇▅▂█▁▃▄▂▁▂▄▁▁▅▇▁▁▁▄▄▂▃▃▁▂▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▅▁▁▇▂▁▁▄▁▁▃▄▂▂▁▄▄▁▃▁▁▁▁▅▁▁▅▁▃▁▁▄▄▁▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.0</td></tr><tr><td>val_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mild-smoke-170</strong>: <a href=\"https://wandb.ai/shayaan-absar/Cross-Lingual-Idiom-Sense-Clustering/runs/1yvkno1i\" target=\"_blank\">https://wandb.ai/shayaan-absar/Cross-Lingual-Idiom-Sense-Clustering/runs/1yvkno1i</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230712_221949-1yvkno1i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "triplet_loss = nn.TripletMarginLoss()\n",
    "\n",
    "def pass_batch(batch):\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tpositive_samples = get_positive_sample(batch)\n",
    "\tnegative_samples = get_negative_sample(batch)\n",
    "\n",
    "\tencodings = model(batch)\n",
    "\tpositive_sample_encodings = model(positive_samples)\n",
    "\tnegative_sample_encodings = model(negative_samples)\n",
    "\n",
    "\tloss = triplet_loss(encodings, positive_sample_encodings, negative_sample_encodings)\n",
    "\treturn loss\n",
    "\n",
    "print(f'Batch size=~{batch_size*batch_accumulation*2}')\n",
    "for i in range(iterations):\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tloss = 0\n",
    "\tfor j in range(batch_accumulation):\n",
    "\t\tbatch = get_batch(train)\n",
    "\t\tloss += pass_batch(batch)\n",
    "\tloss /= batch_accumulation\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tval_loss = pass_batch(get_batch(val))\n",
    "\twandb.log({\"val_loss\": val_loss})\n",
    "\n",
    "\twandb.log({\"loss\": loss})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, 'model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
