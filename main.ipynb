{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h6WigLZWPURc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from torch import nn\n",
        "import torch\n",
        "import csv\n",
        "from random import choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1cP8UKxkPURd",
        "outputId": "1bc8381b-cf0c-484f-8ded-88d23c1648c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([14673, 24])\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "translation_clusters = {}\n",
        "idioms = set()\n",
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "with open('dataset.csv') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        idiom = row['english']\n",
        "        translation = row['german']\n",
        "        idioms.add(idiom)\n",
        "        idioms.add(translation)\n",
        "\n",
        "        if idiom in translation_clusters:\n",
        "            translation_clusters[idiom].add(translation)\n",
        "        else:\n",
        "            translation_clusters[idiom] = {translation}\n",
        "\n",
        "        if translation in translation_clusters:\n",
        "            translation_clusters[translation].add(idiom)\n",
        "        else:\n",
        "            translation_clusters[translation] = {idiom}\n",
        "\n",
        "# Convert idioms to tokenized representations\n",
        "max_length = 0\n",
        "encoder = {}\n",
        "\n",
        "for idiom in idioms:\n",
        "    encoded = tokenizer(idiom, return_tensors='pt', padding=True, truncation=True).input_ids\n",
        "    max_length = max(max_length, encoded.shape[1])\n",
        "    encoder[idiom] = encoded\n",
        "\n",
        "num_idioms = len(idioms)\n",
        "decoder = {}\n",
        "\n",
        "# Generate idiom tensor\n",
        "idiom_tensor = torch.zeros(size=(num_idioms, max_length), dtype=torch.long)\n",
        "for i, idiom in enumerate(idioms):\n",
        "    encoded = encoder[idiom]\n",
        "    idiom_tensor[i, :encoded.shape[1]] = encoded[0]\n",
        "    encoder[idiom] = idiom_tensor[i]\n",
        "    decoder[tuple(idiom_tensor[i].tolist())] = idiom\n",
        "\n",
        "assert decoder[tuple(encoder['makes me feel like'].tolist())] == 'makes me feel like'\n",
        "\n",
        "# Convert idiom clusters to tokenized representations\n",
        "translation_clusters_tokenized = {}\n",
        "for idiom in translation_clusters:\n",
        "    translation_clusters_tokenized[tuple(encoder[idiom].tolist())] = set()\n",
        "    for match in translation_clusters[idiom]:\n",
        "        if tuple(encoder[match].tolist()) == tuple(encoder[idiom].tolist()): continue\n",
        "        translation_clusters_tokenized[tuple(encoder[idiom].tolist())].add(tuple(encoder[match].tolist()))\n",
        "\n",
        "# Print the shape of idiom_tensor\n",
        "print(idiom_tensor.shape)\n",
        "\n",
        "translation_clusters = translation_clusters_tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q3aT53B7PURe"
      },
      "outputs": [],
      "source": [
        "train = idiom_tensor[:int(0.9*num_idioms)]\n",
        "test = idiom_tensor[int(0.9*num_idioms):int(0.95*num_idioms)]\n",
        "val = idiom_tensor[int(0.95*num_idioms)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cKFBxmbOPURe"
      },
      "outputs": [],
      "source": [
        "latent_dimensions = 100\n",
        "device = 'cuda'\n",
        "iterations = 2000\n",
        "learning_rate = 0.01\n",
        "batch_size = 8\n",
        "batch_accumulation = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ekZ5dy91PURe"
      },
      "outputs": [],
      "source": [
        "def tensor_to_set(tensor):\n",
        "    return {tuple(d.tolist()) for d in tensor}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RfR89UMhPURe",
        "outputId": "468a1046-91d5-4095-d92d-b6d03eedf8b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 24])\n"
          ]
        }
      ],
      "source": [
        "def get_batch(data):\n",
        "\tindexes = torch.randint(0, len(data), (batch_size,))\n",
        "\tbatch = set()\n",
        "\tset_data = tensor_to_set(data)\n",
        "\n",
        "\tfor i in indexes:\n",
        "\t\tidiom = data[i]\n",
        "\n",
        "\t\tpossible_idioms = set_data.intersection(translation_clusters[tuple(idiom.tolist())])\n",
        "\n",
        "\t\tif len(possible_idioms) == 0:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tassert not(tuple(idiom.tolist()) in possible_idioms)\n",
        "\n",
        "\t\trandom_cluster_mate = choice(list(possible_idioms))\n",
        "\t\tbatch.add(tuple(random_cluster_mate))\n",
        "\t\tbatch.add(tuple(idiom.tolist()))\n",
        "\n",
        "\tbatch = torch.tensor([list(x) for x in list(batch)]).to(device)\n",
        "\treturn batch\n",
        "\n",
        "print(get_batch(train).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FooH8tw8PURe"
      },
      "outputs": [],
      "source": [
        "def get_positive_sample(data):\n",
        "    positive_samples = []\n",
        "\n",
        "    for anchor in data:\n",
        "        possible_positive = translation_clusters[tuple(anchor.tolist())].intersection(tensor_to_set(data))\n",
        "\n",
        "\n",
        "        chosen = torch.tensor(choice(list(possible_positive)))\n",
        "        positive_samples.append(chosen)\n",
        "\n",
        "    positive_samples = torch.stack(positive_samples).to(device)\n",
        "\n",
        "    return positive_samples\n",
        "\n",
        "\n",
        "def get_negative_sample(data):\n",
        "    negative_samples = []\n",
        "\n",
        "    for anchor in data:\n",
        "        possible_negative = tensor_to_set(data).difference(translation_clusters[tuple(anchor.tolist())])\n",
        "\n",
        "        chosen = torch.tensor(choice(list(possible_negative)))\n",
        "        negative_samples.append(chosen)\n",
        "\n",
        "    negative_samples = torch.stack(negative_samples).to(device)\n",
        "\n",
        "    return negative_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "anrHVnxyPURe"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "\tdef __init__(self, pooling):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.pooling = pooling\n",
        "\t\tself.roberta = AutoModelForMaskedLM.from_pretrained('xlm-roberta-base')\n",
        "\t\tself.output_layer = nn.Linear(250002, latent_dimensions)\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tinput_ids = torch.tensor(input, dtype=torch.long).to(device)\n",
        "\t\tattention_mask = torch.LongTensor(torch.ones(input.shape, dtype=torch.long)).to(device)\n",
        "\t\troberta_logits = self.roberta(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "\t\tif self.pooling == 'average': pooled = torch.mean(roberta_logits, dim=0)\n",
        "\t\tvector_representation = self.output_layer(pooled)\n",
        "\n",
        "\t\treturn vector_representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "meFKOyz6PURf"
      },
      "outputs": [],
      "source": [
        "model = Model(pooling='average')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8TnNErnoPURf",
        "outputId": "655c6133-f8b5-4f31-8549-2ee708e16338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size=~64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-31f85be76457>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = torch.tensor(input, dtype=torch.long).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0131, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.2176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.3983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9994, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9923, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "tensor(0.9868, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-95b1ee515028>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mbatch_accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "triplet_loss = nn.TripletMarginLoss()\n",
        "\n",
        "def pass_batch(batch):\n",
        "\ttorch.cuda.empty_cache()\n",
        "\n",
        "\tpositive_samples = get_positive_sample(batch)\n",
        "\tnegative_samples = get_negative_sample(batch)\n",
        "\n",
        "\tencodings = model(batch)\n",
        "\tpositive_sample_encodings = model(positive_samples)\n",
        "\tnegative_sample_encodings = model(negative_samples)\n",
        "\n",
        "\treturn triplet_loss(encodings, positive_sample_encodings, negative_sample_encodings)\n",
        "\n",
        "print(f'Batch size=~{batch_size*batch_accumulation*2}')\n",
        "for _ in range(iterations):\n",
        "\tloss = 0\n",
        "\tfor __ in range(batch_accumulation):\n",
        "\t\tbatch = get_batch(train)\n",
        "\t\tloss += pass_batch(batch)\n",
        "\tloss /= batch_accumulation\n",
        "\toptimizer.zero_grad()\n",
        "\tloss.backward()\n",
        "\toptimizer.step()\n",
        "\n",
        "\tprint(loss)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}